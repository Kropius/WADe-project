<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <title> RAT - Technical report </title>
    <link rel="stylesheet" href="TechnicalReport.css">
</head>

<body>
    <article>
        <header>
            <h1 style="font-size:250% !important;">
                RAT - Technical report
            </h1>
            <section typeof=sa:AuthorList>
                <h2 style="font-size:150% !important;">
                    Developed by:
                </h2>
                <ul style="list-style-type : none;">
                    <li typeOf="sa:ContributorRole" property="schema:Author">
                        <span typeof="schema:Person">
                            <span property="schema:name">Cioată Matei-Alexandru</span>
                        </span>
                    </li>
                    <li typeOf="sa:ContributorRole" property="schema:Author">
                        <span typeof="schema:Person">
                            <span property="schema:name">Lipan Radu-Matei</span>
                        </span>
                    </li>
                    <li typeOf="sa:ContributorRole" property="schema:Author">
                        <span typeof="schema:Person">
                            <span property="schema:name">Rezmeriță Mihnea-Ioan</span>
                        </span>
                    </li>
                </ul>
            </section>
        </header>
        <section>
            <h2> 1. Description </h2>
            <p> &emsp; &emsp;
                The application Rest Api Interactive Tool(known as RAT) is a web tool that uses natural language
                constructs to interact with different REST APIs based on their OpenApi
                documentation. With this, a human-like(text based/voice) interaction with the given APIs will be
                possible. The end user will have the possibility to select an API, from a predefined list, or upload 
                the OpenAPI specification of one. He will be able to interact in a natural manner with the API using
                the web interface of the application.
            </p>
            <p>
                &emsp; &emsp; The application will be able to determine valid routes, with valid request bodies or even
                query parameters, according to the selected RestApi, based on the user's commands in natural language.
            </p>
        </section>
        <section>
            <h2>2. Usecase</h2>
            <figure typeOf="sa:image">
                    <img src="../ArchitectureAndDesign/use-case.jpg" alt="Usecase diagram">
                    <figcaption style="text-align: center;">Fig. 1: Usecase diagram</figcaption>
            </figure>
            <p>
                &emsp; &emsp; The above diagram displays the possible ways the user can interact with the application:
            </p>
            <ul>
                <li><em>Uploads a valid OpenApi documentation:</em> The user can upload an OpenApi documentation that will
                    be parsed and saved on the server side to facilitate future queries in natural language.</li>
                <li><em>Choose an available API: </em> The user will select a certain API from the given list of previously
                    added APIs. The application will make the future translation based on this API.</li>
                <li><em>Types a request: </em> The user can type any sentence he wants and the application will try to map
                    it to a valid <em>route</em>, <em>body</em>, <em>Rest verb</em> and <em>parameters</em>. </li>
                <li><em>Uses speech recognition in order to send the request: </em> Similar to <em>Types a request</em> but
                    this time, the user speaks aloud his request and the application will transform the recording to text and
                    then will return a valid <em>route</em>, <em>body</em>, <em>Rest verb</em> and <em>parameters</em>. </li>
            </ul>
        </section>
        <section>
            <h2>3. Architecture</h2>
                <figure typeOf="sa:image">
                    <img src="../ArchitectureAndDesign/rat-overview.png" alt="App's Architecture">
                    <figcaption style="text-align: center;">Fig. 2: App's Architecture</figcaption>
            </figure>
            <p> &emsp; &emsp;
                The whole backend application will be built using <a href="https://cloud.google.com/">Google Cloud Services</a>.
            It was chosen over other cloud services providers because of the generous <a href="https://cloud.google.com/free">free tier</a> provided.
            </p>
            <p> &emsp; &emsp;
                The main computation provider will be <a href="https://cloud.google.com/functions">Google Cloud Functions</a>. The FaaS approach will 
            help us better decouple the logic between our endpoints while keeping costs at a minimum with the "pay as you go" approach of such services.
            We will be able to choose our programming language based on what that function needs to do (in example, python for NLP, NodeJs for CRUD operations
            on our noSql Database) without needing to setup a separate server for that language as well. One main drawback of this approach would be the 
            "cold starts" that appear on log traffic but there are workarounds for that: some light weight languages and frameworks like NodeJs or Python have way smaller
            cold starts than others like Java and if that's not enough we can always put in place a warmup system. We ended up using python because we have to work with 
            both Json specification, using a Nlp model and parsing large dictionaries all 3 being jobs for each python3 is very  powerful.
            </p>
            <p> &emsp; &emsp;
            In order to provide a common interface and url for all our functions, we will be using <a href="https://cloud.google.com/api-gateway">Google Cloud API Gateway</a>. This way we have a single point where
            we need to setup the authentication for our api and where we can check that everything specified in <a href="../OpenApi/openapi.yaml">our OpenAPI specification</a>
            is indeed followed accordingly, especially because the API Gateway specification for Google Cloud is also a <a href="https://cloud.google.com/api-gateway/docs/creating-api-config">V2 OpenApi specification</a>.
            </p>
            <p> &emsp; &emsp;
                For storage we will be using <a href="https://cloud.google.com/firestore">Firestore</a>, Google Cloud's main NoSQL solution: Datastore. We'll be going with noSql
            because we have only one type of entities: apis; we don't have any complex relationships between entities besides some child-parent relationships
            (api-endpoints-attributes). Also, the change of one child doesn't need to be reflected in all of its other appearences so we don't have a need for normalisation.
            </p>
            <p> &emsp; &emsp;
                We will also be using <a href="https://cloud.google.com/natural-language">Google Cloud Natural Language Processing API</a> and 
            <a href="https://www.npmjs.com/package/react-speech-recognition">React speech recognition</a> as described in <em><a href="#NLP">4. Natural Language Processing</a></em>
            and <em><a href="#speechRecognition">4.1 Optional Preprocessing: Speech recognition</a></em> respectively.
            </p>
            <p> &emsp; &emsp;
                We need only one page in our frontend app, so using SPA framework is an intuitive solution. We will be using Angular or React for that. Because of their nature,
            SPAs don't need computation power for hosting so we are fine with using <a href="https://cloud.google.com/storage">Google Cloud Storage</a> in order to provide our
            single <em>index.html</em> file and its dependencies to client browsers.
            </p>
        </section>
        <section>
            <h2 id="NLP">4. Natural Language Processing</h2>
            <p> &emsp; &emsp;
				In order to facilitate text/voice interaction with our application, we decided to use Google Cloud’s Natural Language AI. 
				This service will help with the sentence analysis and parsing, providing useful data that takes the program one step closer 
				to an output in the form of a valid REST request. 
			</p>
			<p>
				&emsp; &emsp;
				“The Cloud Natural Language API provides natural language understanding technologies to developers, including sentiment 
				analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis.” (1)
			</p>
			<p>
				&emsp; &emsp;
				The most useful functionality in our case is the syntax analysis, as it inspects the structure of the language. In other 
				words, when the user writes a command in the form of a sentence and passes it to the Natural Language API, it is broken 
				into tokens (words) which are returned along with detailed linguistic information: parts of speech, morphology and most 
				importantly, dependency tree. 
			</p>
			<p>
				&emsp; &emsp;
				All of this data represents the input for our REST request builder algorithm. The main idea of it is to build a syntactic 
				tree using the information from Natural Language API and then to match the key words from the nodes with the input API 
				specification. In order to form the syntactic tree from the sentence, we need to follow a set of rules:
			</p>
			<ul>
                <li>
					<em>Identifying the root: </em> it is usually a verb at imperative mood (also marked with root label by the Natural 
					Language API). For example, this verb can be “Show”, “Create” etc.. Unfortunately, this is not always the case as we can 
					have expressions like “I want to add …” where “want” is the root verb of the sentence, but “add” is the REST correspondent 
					for the POST operation. Therefore, the root of the syntactic tree will be the verb that matches any HTTP operation the most;
				</li>
                <li>
					<em>Identifying the main entity: </em> this step is trivial as the Natural Language API provides a direct dependency 
					between the verb defining the REST operation and the main entity;
				</li>
                <li>
					<em>Identifying the attribute names of an entity: </em>  as we descend the dependency tree of a main entity, all of 
					the nouns represent attribute names (if multiple attributes of the same entity are specified, they will appear on 
					successive levels in the dependency tree);
				</li>
                <li>
					<em>Identifying the attribute values: </em>  Natural Language API provides a direct dependency between nouns and 
					corresponding adjectives and numerals;
				</li>
				<li>
					Parts of speech other than nouns, adjectives, numerals and verbs will be ignored in the tree building process.
				</li>
            </ul>
			<p>
				&emsp; &emsp;
				The syntactic tree is getting close to forming the valid REST request as it offers information about the HTTP verb and 
				the main entities along with their attribute names and values, but there is one more major problem: some words might be 
				unknown. Let’s take an example: “Show me all cats with blue colouring.”. The final tree will have the following form:
			</p>
			<figure typeOf="sa:image" class="ignore-margins">
                <img src="../ArchitectureAndDesign/rat-nlp-algorithm-example.png" alt="RAT Syntactic tree example" style="width:40%">
                <figcaption style="text-align: center;">Fig. 3: RAT - Syntactic tree example</figcaption>
            </figure>
			<p>
				&emsp; &emsp;
				The match for the above request would be GET on path /cats?color=blue. Even though “colouring” resembles “color”, we need a
				way to match these words. A very good candidate for this is WordNet.
			</p>
			<p>
				&emsp; &emsp;
				“WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. 
				Synsets are interlinked by means of conceptual-semantic and lexical relations. WordNet is also freely and publicly available for download. 
				WordNet's structure makes it a useful tool for computational linguistics and natural language processing.” (2) This tool offers the possibility to easily detect 
				synonyms or related words, based on a score. The root verb will always be detected this way. In the beginning of the 
				algorithm, we can set one or multiple default pairs for each HTTP verb (for example: GET - “Retrieve”, POST - [“Create”, "Add"], 
				PUT - “Replace”, DELETE - [“Delete”, "Exterminate", "Remove], PATCH - “Update”). Using WordNet, “Show” will be matched with “Retrieve” with a 
				high score, therefore matching with GET.
			</p>
			<p>
				&emsp; &emsp;
				The given syntactic tree was built using the following output of the Native Language API:
			</p>
			<figure typeOf="sa:image" class="ignore-margins">
                <img src="../ArchitectureAndDesign/google-nlp-output-example.png" alt="Google NLP Output example" style="width: 60%">
                <figcaption style="text-align: center;">Fig. 4: Google NLP - output example</figcaption>
            </figure>

            <h3 id="speechRecognition">4.1 Optional Preprocessing: <em>Speech recognition</em> </h3>
            <p>&emsp; &emsp;
                <p> &emsp; &emsp;
                It is worth mentioning and discussing the speech to text capabilities of our application. The application can take as input, using the user's microphone, some audio recordings and will
                try to translate them into text. We are using <a href="https://www.npmjs.com/package/react-speech-recognition">React speech recognition</a>. As mentioned in the documentation, under the hood, 
                it uses <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API">Web Speech API </a>.
            </p>
            <p> &emsp; &emsp;
                As the documentation says, this library works perfect with Google Chrome. We experimented a bit with Mozilla, but there was no success using it. The <a href= "https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">Speech Recognition documentation</a> states that browsers like Google Chrome, might use some server-based recognition engine, meaning that the speech recognition functionality will not work offline. This is true.
            </p>

			<h3 id="nlpImplementation">4.2 NLP Implementation </h3>
            <p>&emsp; &emsp;
                As mentioned in Chapter 3, our NLP functionality is deployed in a Google Cloud Function. The algorithm executes the following steps:
            </p>
            <ol>
                <li>
                   The input is represented by two components: the <em>text_content</em> representing the user's sentence and the 
				   <em>api_spec_id</em> which is the ID of the API specification to use in the analysis;
                </li>
                <li>
                    Make a call to <a href="https://cloud.google.com/natural-language">Google Cloud Natural Language Processing API</a> using
					the <em>text_content</em> sentence as input to get the necessary NLP information;
                 </li>
                 <li>
                    Make a call to our own API (GET /apis/{id}, where {id} is the received <em>api_spec_id</em> - <a href="#restApi">see Chapter 5 - Rest API</a>)
					to get the parsed OpenAPI specification. This will provide a dictionary with two important keys: <em>allowed_operations</em> 
					(mappings between HTTP verbs and entities) and <em>allowed_attributes</em> (valid attributes for each entity);
                 </li>
				 <li>
					Use the data from steps 2 and 3 to compute the exact steps described in <a href="#NLP"> the beginning of Chapter 4</a>.
				 </li>
				 <li>
					Throw errors in situations like: the matched pair of HTTP verb and entity is not found in the <em>allowed_operations</em>
					list, there was no entity found with high-enough similarity (minimum 60%), the input provided is in the wrong format etc..
				 </li>
				 <li>
					After every step executed successfully, respond with: matched HTTP verb, matched path, query parameter string for GET and 
					DELETE verbs OR request body for POST, PUT and PATCH.
				 </li>
            </ol>
			<h3 id="nlpLimitations">4.3 NLP Algorithm limitations </h3>
			<p>&emsp; &emsp;
                The algorithm used for the RAT project, even though it can parse sentences and match words by their similarity, it also has some limitations:
			</p>
			<ul>
				<li>
					Although <a href="https://cloud.google.com/natural-language">Google Cloud Natural Language Processing API</a> offers multilingual support,
					WordNet is unfortunately limited to the English vocabulary, as stated in <a href="#bibliography"> bibliography, (2)</a>. Therefore, 
					only english sentences can be handled in our project;
				</li>
				<li>
					The entity names and attributes from the API specifications cannot contain symbols like '-', '_' etc.. If they do, WordNet cannot recognize
					them as English words;
				</li>
				<li>
					The algorithm does not support paths having depth > 1 (for example, /felines/{id}/owners). It is limited to only finding
					a main entity and taking it as the path.
				</li>
                <li>
                    The algorithm does not learn from past queries on the same api specification. this is a drawback of using a general use, already trained model like <a href="https://cloud.google.com/natural-language">Google Cloud Natural Language Processing API</a>.
                    Using Google Cloud Natural Language Processing API comes with the advantage that we are getting one of the most powerful and complete Nlp models and that it works without the need for us to retrain it for each new API specification added,
                    but this advantage is also a drawback as we don't have the ability to improve the model for our very specific usecase on subsequent requests. If we find this as a feature in the future we could either try to make our fork of Google's Nlp 
                    model or try to create a new custom model from the ground up, but this would require training for each new Api specification added which could be an unpleasent experience for the end user.
                </li>
			</ul>
        </section>
        <section>
            <h2 id="restApi">5. Rest API</h2>
            <p> &emsp; &emsp;
                The following routes are the ones currently used by for our project:
            </p>
            <ul>
                <li>
                    <b>POST /apis</b> - creates a new Api entry in the database. In this call, the OpenApi specification is parsed and relevant information is saved in Datastore
                    where it will be extracted by the Nlp function and frontend through GET and LIST endpoints;
                </li>
                <li>
                    <b>GET /apis</b> - lists all stored api specifications. By specification, we mean all the data necessary to display
                    the api in our frontend app: api's internal id, api's public url, api's OpenApi specification url. In the future, this should also include
                    and information about how we should authenticate to the respective api (see <a href="#futurePlans">Future Plans</a>);
                </li>
                <li>
                    <b>GET /apis/{id}</b> - returns all data about the api with the given <em>id</em>. This endpoint cannot be used directly by users, it is a dependency of the Nlp function.
                </li>
                <li>
                    <b>POST /apis/{id}/nlp-to-rest </b> - Translate a command given in natural language into a valid REST request. The natural language text
                    will represent the post's body. This text will be processed as described under <em><a href="#NLP">4. Natural Language Processing</a></em> with the help of
                    <a href="https://cloud.google.com/natural-language">Google's Natural Language Processing API</a>. The api will respond with all the elements
                    needed for the client application to create a valid REST request: <em>route</em>, <em>body</em>, <em>Rest verb</em> and <em>parameters</em>.
                </li>
            </ul>
            <p> &emsp; &emsp;
                More details can be found in our <a href="https://github.com/Kropius/WADe-project/blob/main/documentation/OpenApi/openapi.yaml">OpenAPI Specification</a>.
                Please use <a href="https://editor.swagger.io/">Swagger editor</a>  to read more easily.
            </p>
        </section>
        <section>
            <h2>6. Front-end</h2>
            <p> &emsp; &emsp; The user will interact with a single-page application(SPA), built with React. </p>
            <figure typeOf="sa:image" class="ignore-margins">
                <img src="../ArchitectureAndDesign/rat-front-end.png" alt="Frontend Design">
                <figcaption style="text-align: center;">Fig. 5: Front-end concept</figcaption>
            </figure>
            <p>&emsp; &emsp;
                The photo above represents the final version of the UI interface. Each component has a very well defined purpose. The SwaggerUI component was created using 
                <a href="https://www.npmjs.com/package/swagger-ui-react">react-swagger-ui</a>.
            </p>
            <p> &emsp; &emsp;
                The components used are part of <a href="https://www.npmjs.com/package/@awsui/components-react">AWS-UI</a> library.
            </p>
            
        </section>
		<section>
			<h2>7. Test API</h2>
			<p> 
				&emsp; &emsp; 
				In order to easily test and demo our project, we decided to create a simple API that proves the concept of RAT: Bugs, Cats & Weapons API!
				It consists of three main entities: felines, bugs and weapons and each of them are compatible with different combinations of HTTP verbs
				to demonstrate the error handling mechanism from the NLP algorithm:
			</p>
				<ul>
					<li>
						/feline - GET, PUT, PATCH, DELETE
					</li>
					<li>
						/bug - DELETE
					</li>
					<li>
						/weapon - POST, PUT
					</li>
				</ul>
			<p>
				For example, the sentence "Create a cockroach with brown color and six legs.", matching POST on path /bug, will throw an error at the
				NLP function level because the operation is not supported. 
			</p>
			<p>
				The simplicity of this API is brought by the fact that all routes will return a <em>Generic Response</em>, meaning a simple message
				containing important request data: verb, path and parameters.
				Bugs, Cats & Weapons API is sufficient for the scope of this project since it respects the created OPEN API specification. The only missing concept is
				data persistence.
			</p>
			<p>
				For the test API, we used a separate <a href="https://cloud.google.com/api-gateway">Google Cloud API Gateway </a> to ensure 
				authentication and Open API specification compliance, respectively <a href="https://cloud.google.com/functions">Google Cloud Functions</a>
				as computation provider. We can also observe logs to check that the API is indeed being called. 
			</p>
		</section>
		<section>
			<h2 id="futurePlans"> 8. Future plans </h2>
			<p>
				&emsp; &emsp;
				This implementation for the RAT project, even though it satisfies the majority of the specifications, it still needs work on certain areas
				for improvement:
			</p>
			<ul>
				<li>
					Address the NLP limitations mentioned in <a href = "#nlpLimitations">section 4.3</a>. The current algorithm handles the general, most used case: API respecting the REST
                    paradigm with well defined, atomic entities (i.e. <b>no tokens params</b> or custom composed paths like <b>"/pet/findBystatus"</b>) and this is by design. Working around this would imply 
                    adding custom logic for each new API (that cnnot be added fully automatic each time) which would not scale very well (from an effort point of view) and would never assure fully
                    complience anyway. Support for depth > 1 can be implemented by applying custom logic on keyworsds like <b>"of"</b> (i.e. "Give me the owners <b>of</b> cat named Stacy" for <b>"/felines/Stacy/owners"</b>).
                    This also would not be always 100% accurate but would help us in being fully compliant with the REST paradigm, for at least most used use cases.
				</li>
				<li>
					Implement a way to determine API credentials dynamically. The format of what kind of auth is used and who is the trusted checking entity (for Oauth2) is already shown in OpenAPI under <b>"security"</b> and <b>"securityDefinitions"</b> keywords.
                    The exact credentials should be required when adding a new specification to Datastore and, based on the type of auth used, branching logic for each auth type supported by Open API should be supported at front-end level before creating the request to the real API.
                    This is a really nice possible future improvement that wasn't added yet, as this project was more of a Proof Of Concept for using Nlp to transform natural sentences to REST queries;
				</li>
				<li>
					Auth mechanism for our own APIs, such that not everyone can add new specifications annd so that different users can have different APIs stored under their account;
				</li>
			</ul>
		</section>
        <section>
            <h2 id="bibliography">9. Bibliography</h2>
            <ol>
				<li> <a href="https://cloud.google.com/natural-language/docs/basics">Google Cloud - Natural Language API</a> </li>
				<li> <a href="https://wordnet.princeton.edu/">WordNet - Word Association tool</a> </li>
				<li> <a href="https://arxiv.org/pdf/2007.15280.pdf">PHOTON: A Robust Cross-Domain Text-to-SQL System - Jichuan Zeng, Xi Victoria Lin,
				Caiming Xiong, Richard Socher, Michael R. Lyu, Irwin King, Steven C.H. Hoi</a> </li>
				<li> <a href="https://cloud.google.com/api-gateway/docs">Google Cloud - API Gateway </a> </li>
				<li> <a href="https://cloud.google.com/functions#section-4">Google Cloud Functions</a> </li>
				<li> <a href="https://cloud.google.com/storage#section-4">Google Cloud Storage</a> </li>
				<li> <a href="https://firebase.google.com/products/firestore">Cloud Firestore</a> </li>
				<li> <a href="https://cloud.google.com/speech-to-text#section-6">Google Cloud - Speech to Text</a> </li>
                <li> <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">Speech Recognition </a> </li>
			</ol>
        </section>

    </article>
</body>

</html>